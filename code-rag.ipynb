{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6908d41",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from typing import List, Dict\n",
    "\n",
    "def hash_file(content: str) -> str:\n",
    "    \"\"\"Generate a SHA-256 hash for the given content.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    sha256.update(content.encode('utf-8'))\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def read_file(path: str) -> str:\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "    \n",
    "def write_file(path: str, content: str) -> None:\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def read_files_recursive(\n",
    "        directory: str, \n",
    "        extensions: List[str], \n",
    "        ignore_folders: List[str]=None\n",
    ") -> List[str]:\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(directory):\n",
    "        if ignore_folders and any(ignored in root for ignored in ignore_folders):\n",
    "            continue\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in extensions):\n",
    "                files.append(os.path.join(root, filename))\n",
    "    return files\n",
    "\n",
    "class Document(Dict):\n",
    "    \"\"\"A document that contains metadata and content.\"\"\"\n",
    "    path: str\n",
    "    directory: str\n",
    "    name: str\n",
    "    content: str\n",
    "    extension: str\n",
    "    size: int\n",
    "    hash: str\n",
    "\n",
    "def fetch_documents(folder_path, extensions, ignore_folders=None) -> List[Document]:\n",
    "    files = read_files_recursive(folder_path, extensions, ignore_folders)\n",
    "    documents: List[Document] = []\n",
    "    for file in files:\n",
    "        content = read_file(file)\n",
    "        doc = Document(\n",
    "            path=file,\n",
    "            directory=os.path.dirname(file),\n",
    "            name=os.path.basename(file),\n",
    "            content=content,\n",
    "            extension=os.path.splitext(file)[1],\n",
    "            size=os.path.getsize(file),\n",
    "            hash=hash_file(content)\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1642a",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19fcf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "model_name = \"mistral:7b-instruct\"\n",
    "lm = dspy.LM(model=model_name, api_base=\"http://localhost:11434\", api_key=\"\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f3f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def embed_text(text: str) -> List[float]:\n",
    "    \"\"\"Embed the given text using Ollama's embedding model.\"\"\"\n",
    "    response = ollama.embed(model=\"nomic-embed-text\", input=text)\n",
    "    return response['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dacf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm interface tools\n",
    "import dspy\n",
    "\n",
    "class CodeSummary(dspy.Signature):\n",
    "    \"\"\"Write a detailed summary of the given code including low level details which explains the steps involved.\"\"\"\n",
    "    code = dspy.InputField(desc=\"The code to be summarized.\")\n",
    "    summary = dspy.OutputField(desc=\"Summary of the code with all the details that make up the logic.\")\n",
    "\n",
    "summarize_code = dspy.ChainOfThought(CodeSummary)\n",
    "\n",
    "condense_summary = dspy.ChainOfThought(\"document -> summary\")\n",
    "\n",
    "ask_question = dspy.ChainOfThought(\"context, question -> response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162614ce",
   "metadata": {},
   "source": [
    "### RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45e4a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "class SummaryDocument(Dict):\n",
    "    \"\"\"A document that contains a summary of the code.\"\"\"\n",
    "    target_docs: List[Document]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea998b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[SummaryDocument], \n",
    "    level: int = 1, \n",
    "    n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[SummaryDocument], summaries to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
